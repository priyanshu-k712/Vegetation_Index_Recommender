{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2634fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a96e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "candidates = [\n",
    "    os.path.join(cwd, 'code-dataset2.py'),\n",
    "]\n",
    "helper_path = None\n",
    "for p in candidates:\n",
    "    if os.path.exists(p):\n",
    "        helper_path = p\n",
    "        break\n",
    "\n",
    "if helper_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find 'code-dataset2.py' or 'code_dataset2.py' in the current working directory.\\n\"\n",
    "        f\"Tried: {candidates}\\nPlease upload the helper file to the notebook folder.\"\n",
    "    )\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"code_dataset2_mod\", helper_path)\n",
    "code_dataset2_mod = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(code_dataset2_mod)\n",
    "\n",
    "# extract required functions (will raise a clear error if missing)\n",
    "required_funcs = ['generate_and_load_dataset_from_files', 'build_custom_cnn']\n",
    "missing = [f for f in required_funcs if not hasattr(code_dataset2_mod, f)]\n",
    "if missing:\n",
    "    raise AttributeError(f\"Missing expected function(s) in helper module: {missing}. \"\n",
    "                         \"Open code-dataset2.py and make sure the functions are defined.\")\n",
    "\n",
    "generate_and_load_dataset_from_files = code_dataset2_mod.generate_and_load_dataset_from_files\n",
    "build_custom_cnn = code_dataset2_mod.build_custom_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad058d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '/content/drive/MyDrive/Dataset_2'  \n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "INITIAL_LR = 0.001\n",
    "MIN_REQUIRED_SAMPLES = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(cwd, 'pretrain_data')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e15363",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset from:\", DATASET_DIR)\n",
    "X, y = generate_and_load_dataset_from_files(DATASET_DIR)\n",
    "\n",
    "if not hasattr(X, 'size') or not hasattr(y, 'size'):\n",
    "    raise ValueError(\"generate_and_load_dataset_from_files must return numpy arrays (X, y).\")\n",
    "\n",
    "if X.size == 0 or y.size == 0:\n",
    "    raise SystemExit(\"No data loaded. Check DATASET_DIR and helper function.\")\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "print(f\"Loaded {n_samples} samples.\")\n",
    "if n_samples < MIN_REQUIRED_SAMPLES:\n",
    "    raise SystemExit(f\"Need at least {MIN_REQUIRED_SAMPLES} images to proceed (found {n_samples}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a44092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "input_shape = X_train.shape[1:]\n",
    "model = build_custom_cnn(input_shape)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_during_training_path = os.path.join(out_dir, 'best_model_vndvi_best.h5')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(filepath=best_during_training_path,\n",
    "                                   monitor='val_loss', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-5, verbose=1)\n",
    "callbacks = [early_stopping, model_checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test MSE: {loss:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "# save full model (HDF5)\n",
    "h5_path = os.path.join(out_dir, f'best_model_vndvi_{timestamp}.h5')\n",
    "model.save(h5_path)\n",
    "print(\"Saved full model (h5) to:\", h5_path)\n",
    "\n",
    "# save JSON + weights via pickle\n",
    "pkl_path = os.path.join(out_dir, f'best_model_vndvi_pickle_{timestamp}.pkl')\n",
    "model_dict = {\n",
    "    'model_json': model.to_json(),\n",
    "    'weights': model.get_weights()\n",
    "}\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(model_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Saved model JSON+weights (pickle) to:\", pkl_path)\n",
    "\n",
    "# list files in pretrain_data to confirm\n",
    "print(\"\\nFiles in\", out_dir, \":\")\n",
    "for fn in sorted(os.listdir(out_dir)):\n",
    "    print(\" -\", fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
